{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6743265-6f14-46a1-ae89-e5c0878b1833",
   "metadata": {},
   "source": [
    "# Baseline OctoShop Pipeline\n",
    "In this iPython Notebook, \n",
    "* We'll test a newly minted SDXL container locally\n",
    "* We'll then provide instructions to launch it on OctoAI compute services\n",
    "* You'll go ahead and learn to generate different prompts to SDXL to get amazing images\n",
    "* Finally you'll test your SDXL endpoint within an \"Baseline OctoShop\" pipeline, composed of a CLIP Interrogator, Llama 2, and SDXL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed07378-f3c6-4d57-9303-6852e1e1041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some useful libraries\n",
    "import requests\n",
    "import json\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from base64 import b64encode, b64decode\n",
    "from IPython.display import display\n",
    "\n",
    "# Let's import the OctoAI Python SDK\n",
    "from octoai.client import Client\n",
    "\n",
    "# A helper function that reads a PIL Image objects and returns a base 64 encoded string\n",
    "def encode_image(image: Image) -> str:\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=\"png\")\n",
    "    im_base64 = b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    return im_base64\n",
    "\n",
    "# A helper function that reads a base64 encoded string and returns a PIL Image object\n",
    "def decode_image(image_str: str) -> Image:\n",
    "    return Image.open(BytesIO(b64decode(image_str)))\n",
    "\n",
    "# Initialize the OctoAI Client\n",
    "# This will make it easier to interface with the model containers\n",
    "client = Client()\n",
    "\n",
    "# Please ignore the warning below - we don't need the token because we've enabled public access on our endpoints\n",
    "# WARNING:root:OCTOAI_TOKEN environment variable is not set. You won't be able to reach OctoAI endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e33674-f573-471f-90b1-5550be011cca",
   "metadata": {},
   "source": [
    "## A. Test your SDXL container locally\n",
    "Make sure you've completed Sections 1 and 2 of Lab 1 described in the README.md.\n",
    "\n",
    "As a recap, the SDXL model container takes as input a dictionary with the following keys:\n",
    "* `prompt` (string) - the SDXL text prompt\n",
    "* `negative_prompt` (string) - the SDXL text prompt\n",
    "* `guidance_scale` (float) - the guidance scale (a.k.a. the configuration scale) of SDXL\n",
    "* `num_inference_steps` (int) - the number of SDXL denoising steps\n",
    "* `width` (int) - the width of the SDXL output image\n",
    "* `height` (int) - the height of the SDXL output image\n",
    "* `seed` (int) - seed of the image generation\n",
    "\n",
    "SDXL model container returns the following as outputs:\n",
    "* `image` (string) - a base64-encoded image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa91e6-077c-4cc0-a54e-1353ca37ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prepare our SDXL inference endpoint payload\n",
    "SDXL_payload = {\n",
    "    \"prompt\": \"a photo of an octopus playing chess\",\n",
    "    \"negative_prompt\": \"blurry photo, distortion, low-res, bad quality\",\n",
    "    \"num_inference_steps\": 20,\n",
    "    \"guidance_scale\": 7.5,\n",
    "    \"width\": 1024,\n",
    "    \"height\": 1024,\n",
    "    \"seed\": 1\n",
    "}\n",
    "\n",
    "# Run inference on the OctoAI SDXL model container running locally\n",
    "output = client.infer(\n",
    "    endpoint_url=\"http://localhost:8080/predict\",\n",
    "    inputs=SDXL_payload\n",
    ")\n",
    "\n",
    "# Get the base64 encoded image string\n",
    "image_string = output[\"completion\"][\"image\"]\n",
    "\n",
    "# Convert to a PIL image\n",
    "sdxl_image = decode_image(image_string)\n",
    "\n",
    "# Display your masterpiece!\n",
    "display(sdxl_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b39f2f-5f15-4657-a5b4-21882aeaba07",
   "metadata": {},
   "source": [
    "## B. Upload the image to your DockerHub\n",
    "Now sign onto your DockerHub in a browser: https://hub.docker.com/\n",
    "\n",
    "Create a repository by clicking on the `Create repository` blue button. Name it `dockercon-sdxl`, and provide a short description as you see fit. Leave it public. Hit the `Create` blue button.\n",
    "\n",
    "Once that's done, note the full path to the repo, as `<dockerhub-username>/dockercon-sdxl`.\n",
    "\n",
    "Start a new ssh session from your laptop terminal or ssh client:\n",
    "\n",
    "```bash\n",
    "ssh -i \"dockercon23-attendee.pem\" ubuntu@ec2-X-X-X-X.compute-1.amazonaws.com\n",
    "```\n",
    "\n",
    "Under `lab1/sdxl`, run the following to tag the Docker image we just tested to a versioned image we'll push to the newly created DockerHub repository.\n",
    "\n",
    "```bash\n",
    "cd ~/dockercon23-octoai/lab1/sdxl\n",
    "docker tag sdxl:latest <dockerhub-username>/dockercon-sdxl:v0.1.0\n",
    "```\n",
    "\n",
    "Then push the tagged SDXL model image!\n",
    "\n",
    "```bash\n",
    "docker push <dockerhub-username>/dockercon-sdxl:v0.1.0\n",
    "```\n",
    "\n",
    "This should take about 10 minutes given that the image is quite voluminous (that's pretty common for Generative AI models with their huge sets of weights!).\n",
    "\n",
    "Refresh the dockerhub page of the sdxl repository, and you should see a new `v0.1.0` image that was uploaded just now!\n",
    "\n",
    "![Docker](https://raw.githubusercontent.com/vegaluisjose/blob/main/docker_sdxl.png)\n",
    "\n",
    "If you don't feel like waiting for the full image to upload, you can go ahead and use this image that we've prebuilt for step C: [tmoreau89octo/dockercon-sdxl:v0.1.0](https://hub.docker.com/layers/tmoreau89octo/dockercon-sdxl/v0.1.0/images/sha256-b6d5d858e98fc9fb6482a52d7b8ec47a73c631614a5a99ec8e890bb83f15a277?context=repo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b36699-65b7-4438-b3d0-e45a67c5aab7",
   "metadata": {},
   "source": [
    "## C. Deploy the SDXL image on an OctoAI endpoint\n",
    "Sign onto your OctoAI account in a browser: https://octoai.cloud/endpoints\n",
    "\n",
    "Click on the `Create a Custom Endpoint` blue button.\n",
    "\n",
    "Name your endpoint, e.g. `dockercon23-sdxl`.\n",
    "\n",
    "Under the `Model container` details:\n",
    "* Set the `Container image` to `<dockerhub-username>/dockercon-sdxl:v0.1.0`\n",
    "* Leave the `Container port` to its default `8080` value.\n",
    "* Leave the `Registry credential` to `Public`.\n",
    "* Set the `Health check path` to `/healthcheck`.\n",
    "* Enable public access by toggling the switch (usually we'd recommend leaving it disabled but for the purpose of this lab, let's keep things simple).\n",
    "* No need to specify secrets.\n",
    "* No need to specify environment variables.\n",
    "\n",
    "Under `Hardware tier`, select `Medium`. The `Small` tier is unfortunately not powerful enough to run SDXL.\n",
    "\n",
    "Under `Configure autoscaling`:\n",
    "* Change Min replicas to `1`. This will ensure at least one replica remains up and running.\n",
    "* Change Max replicas to `1`. This will ensure no more than one replica remains up and running.\n",
    "* Leave the timeout to `300` seconds.\n",
    "\n",
    "![OctoAI](https://raw.githubusercontent.com/vegaluisjose/blob/main/octoai_sdxl.png)\n",
    "\n",
    "Now scroll up and hit the `Create` button at the top right of the page!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478cb6b5-f6d4-4a93-ad7f-a1bf5b7ce61a",
   "metadata": {},
   "source": [
    "## D. Manage your SDXL OctoAI endpoint\n",
    "\n",
    "The endpoint will need to \"cold start\" and this could take about 10 minutes.\n",
    "\n",
    "On the OctoAI endpoint Info view, you'll know that your endpoint is warming up because the endpoint status will be set to `Starting`. You can click on the blinking square under `Replicas` to see that the container image is being pulled onto the Octoai endpoint replica.\n",
    "\n",
    "Once the status is set to `Running`, you can view the logs by clicking on the `View logs` button.\n",
    "\n",
    "You can at any time ramp your endpoint down, by clicking on the `Pause endpoint` button.\n",
    "\n",
    "***Last but not least, save the `Endpoint URL` that's displayed in the `dockercon23-sdxl` model endpoint Info view. We'll use it in the next step, and when you launch your Discord bot***\n",
    "\n",
    "![OctoAICreated](https://raw.githubusercontent.com/vegaluisjose/blob/main/octoai_sdxl_created.png)\n",
    "\n",
    "You can go to Section E to experiment with SDXL styles while you wait for your endpoint to warm up, as it doesn't require the SDXL OctoAI endpoint to be up and running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e92938-3ba2-4898-a450-5a1f6c005945",
   "metadata": {},
   "source": [
    "## E. Play with SDXL Styles!\n",
    "\n",
    "While you wait for steps B and D which each take a while, you can experiment with SDXL styles.\n",
    "\n",
    "The key to getting beautiful, stylized images with SDXL is to provide the right prompt and negative prompt. We refer to this process as \"prompt engineering\".\n",
    "\n",
    "You can find inspiration on what kinds of beautiful images you can generate with SDXL by browsing through this beautiful image gallery: https://moby-dock.vercel.app/\n",
    "\n",
    "SDXL styles prompts can be accessed from this open source project: \n",
    "* https://github.com/twri/sdxl_prompt_styler/blob/main/sdxl_styles_sai.json\n",
    "* https://github.com/twri/sdxl_prompt_styler/blob/main/sdxl_styles_twri.json\n",
    "\n",
    "To use these styles use the code below to perform some prompt engineering! Play with different styles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2321c48-79af-4f51-9629-c89ce77a8665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original SDXL prompt\n",
    "prompt = \"a photo of an octopus playing chess\"\n",
    "\n",
    "# We copy the style entry for the game retro arcade from https://github.com/twri/sdxl_prompt_styler/blob/0664f1e378661888bc0f0fc101c98a1a696e658e/sdxl_styles_twri.json#L222-L226\n",
    "sdxl_style = {\n",
    "    \"name\": \"game-retro arcade\",\n",
    "    \"prompt\": \"retro arcade style {prompt} . 8-bit, pixelated, vibrant, classic video game, old school gaming, reminiscent of 80s and 90s arcade games\",\n",
    "    \"negative_prompt\": \"modern, ultra-high resolution, photorealistic, 3D\"\n",
    "}\n",
    "\n",
    "# Let's go ahead and apply the style to our SDXL payload\n",
    "SDXL_payload = {\n",
    "    \"prompt\": sdxl_style[\"prompt\"].replace(\"{prompt}\", prompt),\n",
    "    \"negative_prompt\": sdxl_style[\"negative_prompt\"],\n",
    "    \"num_inference_steps\": 20,\n",
    "    \"guidance_scale\": 7.5,\n",
    "    \"width\": 1024,\n",
    "    \"height\": 1024,\n",
    "    \"seed\": 1\n",
    "}\n",
    "\n",
    "# Run inference on the OctoAI SDXL model container running locally\n",
    "output = client.infer(\n",
    "    endpoint_url=\"http://localhost:8080/predict\",\n",
    "    inputs=SDXL_payload\n",
    ")\n",
    "\n",
    "# Get the base64 encoded image string\n",
    "image_string = output[\"completion\"][\"image\"]\n",
    "\n",
    "# Convert to a PIL image\n",
    "sdxl_image = decode_image(image_string)\n",
    "\n",
    "# Display your masterpiece!\n",
    "display(sdxl_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1c18e-de6c-4530-8d71-dc890d3a5f82",
   "metadata": {},
   "source": [
    "## E. Test your SDXL container served on an OctoAI endpoint\n",
    "In this step, we'll test the SDXL container in the exact same way as we did when we ran the container locally on the AWS dev instance, except that now we'll be sending a POST request to a remote endpoint.\n",
    "\n",
    "You'll need to change the SDXL endpoint URL from `http://localhost:8080` to your unique endpoint URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9c7bf-ae44-478f-8b99-227d708f6675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: Replace \"http://localhost:8080\" with your OctoAI SDXL endpoint URL below\n",
    "sdxl_endpoint_url = \"http://localhost:8080\"\n",
    "# Make sure you've overwritten the URL!!!\n",
    "assert sdxl_endpoint_url != \"http://localhost:8080\"\n",
    "\n",
    "# Run inference on the OctoAI SDXL model container running locally\n",
    "output = client.infer(\n",
    "    endpoint_url=\"{}/predict\".format(sdxl_endpoint_url),\n",
    "    inputs=SDXL_payload\n",
    ")\n",
    "\n",
    "# Get the base64 encoded image string\n",
    "image_string = output[\"completion\"][\"image\"]\n",
    "\n",
    "# Convert to a PIL image\n",
    "sdxl_image = decode_image(image_string)\n",
    "\n",
    "# Display your masterpiece!\n",
    "display(sdxl_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76f5724-bf6e-451a-bf08-f3d4c466e4e9",
   "metadata": {},
   "source": [
    "## F. Test the CLIP Interrogator Model on the Docker Logo\n",
    "Now that we've tested that our SDXL model endpoint works we'll proceed to testing the other models used in the \"Baseline OctoShop\" pipeline. Let's start with the CLIP Interrogator model first.\n",
    "\n",
    "This model takes in an image, and produces a text-based description of the image. Think of it as reverse Stable Diffusion, which takes in text and produces an image.\n",
    "\n",
    "Note that for this DockerCon23 workshop, we've pre-allocated a CLIP Interrogator endpoint pool available at the following URL: https://dockercon23-clip-4jkxk521l3v1.octoai.run\n",
    "You don't need to do anything!\n",
    "\n",
    "**If you try this tutorial after October 3rd 2023**, this CLIP endpoint will be taken down. You can still create and manage your own by going on https://octoai.cloud/templates\n",
    "* Under the list of Example Models, select `Image captioning (CLIP)`, and click on `Clone`\n",
    "* Name your endpoint, set Min replicas and Max replicas to 1, and Enable public access with the toggle\n",
    "* You can then launch your endpoint by hitting `Clone`\n",
    "* Once the endpoint is running, you can copy the URL of the OctoAI endpoint, and set `clip_endpoint_url` to it below\n",
    "\n",
    "![OctoAICreated](https://raw.githubusercontent.com/vegaluisjose/blob/main/octoai_clip.png)\n",
    "\n",
    "We'll test the CLIP Interrogator in a Image to Text and Text to Image workflow using SDXL to see what we end up with! Let's try this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b7176-a0bd-4397-ab56-7b353ba4c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide a CLIP Interrogator endpoint for you, but with OctoAI you can launch your own by using one of OctoAI's many model templates\n",
    "clip_endpoint_url = \"https://dockercon23-clip-4jkxk521l3v1.octoai.run\"\n",
    "\n",
    "# Let's grab the Docker logo\n",
    "r = requests.get('https://raw.githubusercontent.com/vegaluisjose/blob/main/docker.jpeg')\n",
    "image = Image.open(BytesIO(r.content))\n",
    "\n",
    "# Display the Docker logo\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb3f47-acb5-4d4c-b968-773aec1cccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CLIP interrogator request is simple\n",
    "# We use CLIP interrogator's fast mode to get a response quickly\n",
    "clip_request = {\n",
    "    \"mode\": \"fast\",\n",
    "    \"image\": encode_image(image),\n",
    "}\n",
    "\n",
    "# Run inference on the CLIP model container running locally\n",
    "output = client.infer(\n",
    "    endpoint_url=\"{}/predict\".format(clip_endpoint_url),\n",
    "    inputs=clip_request\n",
    ")\n",
    "\n",
    "# Get the labes from the output dictionary\n",
    "clip_labels = output[\"completion\"][\"labels\"]\n",
    "\n",
    "# Print the CLIP labels\n",
    "print(\"Here is what CLIP interrogator sees: {}\".format(clip_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0904cc5-6233-4a86-a194-6c58995879bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just keep the first CLIP comma-separated answer here to keep the prompt simple\n",
    "clip_labels = clip_labels.split(',')[0]\n",
    "print(\"Shortened CLIP interrogator labels: {}\".format(clip_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deabbc3d-4181-49d4-be3f-a70eee30ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next let's try to generate a cinematic SDXL image out of those CLIP labels!\n",
    "\n",
    "# Let's use a cinematic style: https://github.com/twri/sdxl_prompt_styler/blob/0664f1e378661888bc0f0fc101c98a1a696e658e/sdxl_styles_sai.json#L17-L21\n",
    "sdxl_style = {\n",
    "    \"name\": \"sai-cinematic\",\n",
    "    \"prompt\": \"cinematic film still {prompt} . shallow depth of field, vignette, highly detailed, high budget, bokeh, cinemascope, moody, epic, gorgeous, film grain, grainy\",\n",
    "    \"negative_prompt\": \"anime, cartoon, graphic, text, painting, crayon, graphite, abstract, glitch, deformed, mutated, ugly, disfigured\"\n",
    "}\n",
    "\n",
    "# Update the SDXL prompts\n",
    "SDXL_payload[\"prompt\"] = sdxl_style[\"prompt\"].replace(\"{prompt}\", clip_labels)\n",
    "SDXL_payload[\"negative_prompt\"] = sdxl_style[\"negative_prompt\"]\n",
    "\n",
    "# Run inference on the OctoAI SDXL model container running locally\n",
    "output = client.infer(\n",
    "    endpoint_url=\"{}/predict\".format(sdxl_endpoint_url),\n",
    "    inputs=SDXL_payload\n",
    ")\n",
    "\n",
    "# Get the base64 encoded image string\n",
    "image_string = output[\"completion\"][\"image\"]\n",
    "\n",
    "# Convert to a PIL image\n",
    "sdxl_image = decode_image(image_string)\n",
    "\n",
    "# Display your masterpiece!\n",
    "display(sdxl_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57cead5-0b53-4b0c-82ed-bef179b50de3",
   "metadata": {},
   "source": [
    "## G. Using LLMs to manipulate the Docker Logo based on a User Prompt\n",
    "In section E, we turned the Docker logo into a hyper realistic photo by using a CLIP interrogator model to obtain a text-based explanation of what the logo was, then fed that text into and SDXL model to obtain a hyperrealistic version of the logo.\n",
    "\n",
    "In the next section we'll use an LLM, specifically Llama 2-7B in order to alter the image we're generating even more. We'll base ourselves on a user prompt, which asks to set the image on the moon. We'll feed that prompt and the CLIP labels into the LLM in order to obtain a richer picture of our whale in space to then feed into SDXL.\n",
    "\n",
    "Here again, for this DockerCon23 workshop, we've pre-allocated a Llama 2 model endpoint pool available at the following URL: https://dockercon23-llama2-4jkxk521l3v1.octoai.run\n",
    "You don't need to do anything!\n",
    "\n",
    "**If you try this tutorial after October 3rd 2023** however, this Llama 2 endpoint will be taken down. You can still create and manage your own by going on https://octoai.cloud/templates\n",
    "* Under the list of Example Models, click on `LLama 2 7B Chat`\n",
    "* Then you can hit the big `Clone Template` button\n",
    "* Once the endpoint is running, you can copy the URL of the OctoAI endpoint, and set `llama2_endpoint_url` to it below\n",
    "\n",
    "![OctoAICreated](https://raw.githubusercontent.com/vegaluisjose/blob/main/octoai_llama2.png)\n",
    "\n",
    "Let's go ahead and learn how to use the Llama 2 model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c6e2e-8317-4348-88a5-f42ac5a8af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide a Llama 2 endpoint for you, but with OctoAI you can launch your own by using one of OctoAI's many model templates\n",
    "llama2_endpoint_url = \"https://dockercon23-llama2-4jkxk521l3v1.octoai.run/v1/chat/completions\"\n",
    "\n",
    "# Let's start with the user prompt which we'll set as follows\n",
    "user_prompt = \"set in outer space\"\n",
    "\n",
    "# Now let's engineer a prompt to set what CLIP Interrogator sees on the moon\n",
    "llama_prompt = \"\\\n",
    "### Instruction: In a single sentence, {}: {}\\n\\\n",
    "### Response:\".format(user_prompt, clip_labels)\n",
    "\n",
    "# Let's print the Llama 2 prompt before we feed it into Llama 2\n",
    "print(\"Llama 2 prompt:\\n{}\".format(llama_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baed5107-bfcb-4e06-977b-a8f1bb24ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's prepare an LLM prompt that describes the tasks to accomplish to our Llama 2 model\n",
    "# You can leave the parameters below as-is.\n",
    "llama_inputs = {\n",
    "    \"model\": \"llama-2-7b-chat\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"{}\".format(llama_prompt)\n",
    "        }\n",
    "    ],\n",
    "    \"stream\": False,\n",
    "    \"max_tokens\": 70,\n",
    "    \"temperature\": 0\n",
    "}\n",
    "\n",
    "# Send to LLAMA endpoint and do some post processing on the response stream\n",
    "outputs = client.infer(endpoint_url=llama2_endpoint_url, inputs=llama_inputs)\n",
    "\n",
    "# Get the Llama 2 output\n",
    "llama2_text = outputs.get('choices')[0].get(\"message\").get('content')\n",
    "\n",
    "# Print the Llama 2 story\n",
    "print(\"Llama 2 generated text below:\\n{}\".format(llama2_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245ec7e3-0d08-4893-9e20-a4e0253c0df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next let's feed this Llama 2 generated story into SDXL\n",
    "SDXL_payload[\"prompt\"] = sdxl_style[\"prompt\"].replace(\"{prompt}\", llama2_text)\n",
    "\n",
    "# Run inference on the OctoAI SDXL model container running locally\n",
    "output = client.infer(\n",
    "    endpoint_url=\"{}/predict\".format(sdxl_endpoint_url),\n",
    "    inputs=SDXL_payload\n",
    ")\n",
    "\n",
    "# Get the base64 encoded image string\n",
    "image_string = output[\"completion\"][\"image\"]\n",
    "\n",
    "# Convert to a PIL image\n",
    "sdxl_image = decode_image(image_string)\n",
    "\n",
    "# Display your masterpiece!\n",
    "display(sdxl_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da55fb9e-d0a6-4b5b-9573-f772979a6abd",
   "metadata": {},
   "source": [
    "## H. Let's recap through the entire Baseline OctoShop workflow now\n",
    "\n",
    "Now that we've introduced each model consecutively, let's tie it all together into the OctoShop workflow.\n",
    "\n",
    "1. User provides an image as input (docker logo) and a prompt (set in space).\n",
    "\n",
    "2. Image goes through CLIP interrogator and produces a text description of the image.\n",
    "\n",
    "3. The user prompt gets fed along with the CLIP interrogator description into Llama 2 that describes a new scene.\n",
    "\n",
    "4. That textual description of the scene is fed into SDXL to generate a brand new photo of the Docker logo set in space!\n",
    "\n",
    "We've provided the whole flow below of OctoShop for you to play around with the Generative AI workflow! \n",
    "* Try changing the URL of the input image to a different image!\n",
    "* Try changing the user prompt to a different prompt!\n",
    "* Try changing the user style to a different style!\n",
    "* Try changing any combination of the above at the same time!\n",
    "* And feel free to tweak the various settings to familiarize yourself a bit more to the different models that are being invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e6969d-47d2-4efd-b9b4-9911695e4696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the OctoShop as a self-contained function\n",
    "def octoshop(image: Image, user_prompt: str, user_style: dict) -> (Image, str, str):\n",
    "\n",
    "    # OctoAI endpoint URLs\n",
    "    clip_endpoint_url = \"https://dockercon23-clip-4jkxk521l3v1.octoai.run\"\n",
    "    llama2_endpoint_url = \"https://dockercon23-llama2-4jkxk521l3v1.octoai.run/v1/chat/completions\"\n",
    "    sdxl_endpoint_url = \"http://localhost:8080\" # ADD_YOUR_SDXL_ENDPOINT_URL_HERE\n",
    "    assert sdxl_endpoint_url != \"http://localhost:8080\"\n",
    "\n",
    "    # STEP 1\n",
    "    # Feed that image into CLIP interrogator\n",
    "    clip_request = {\n",
    "        \"mode\": \"fast\",\n",
    "        \"image\": encode_image(image),\n",
    "    }\n",
    "    output = client.infer(\n",
    "        endpoint_url=\"{}/predict\".format(clip_endpoint_url),\n",
    "        inputs=clip_request\n",
    "    )\n",
    "    clip_labels = output[\"completion\"][\"labels\"]\n",
    "    clip_labels = clip_labels.split(',')[0]\n",
    "\n",
    "    # STEP 2\n",
    "    # Feed that CLIP label and the user prompt into a LLAMA model\n",
    "    llama_prompt = \"\\\n",
    "    ### Instruction: In a single sentence, {}: {}\\\n",
    "    ### Response:\".format(user_prompt, clip_labels)\n",
    "    llama_inputs = {\n",
    "        \"model\": \"llama-2-7b-chat\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"{}\".format(llama_prompt)\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"max_tokens\": 70,\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "    outputs = client.infer(endpoint_url=llama2_endpoint_url, inputs=llama_inputs)\n",
    "    llama2_text = outputs.get('choices')[0].get(\"message\").get('content')\n",
    "\n",
    "    # STEP 3\n",
    "    # Feed the Llama 2 text into the SDXL model\n",
    "    SDXL_payload = {\n",
    "        \"prompt\": user_style[\"prompt\"].replace(\"{prompt}\", llama2_text),\n",
    "        \"negative_prompt\": user_style[\"negative_prompt\"],\n",
    "        \"num_inference_steps\": 20,\n",
    "        \"guidance_scale\": 7.5,\n",
    "        \"width\": 1024,\n",
    "        \"height\": 1024,\n",
    "        \"seed\": 1\n",
    "    }\n",
    "    # Run inference on the OctoAI SDXL model container running locally\n",
    "    output = client.infer(\n",
    "        endpoint_url=\"{}/predict\".format(sdxl_endpoint_url),\n",
    "        inputs=SDXL_payload\n",
    "    )\n",
    "    image_string = output[\"completion\"][\"image\"]\n",
    "    sdxl_image = decode_image(image_string)\n",
    "\n",
    "    return sdxl_image, clip_labels, llama2_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a01520-08ae-48e4-9bdd-80ea649ee1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the to the image URL\n",
    "image_url = 'https://raw.githubusercontent.com/vegaluisjose/blob/main/docker.jpeg'\n",
    "\n",
    "# Process encode the input image into a string\n",
    "r = requests.get(image_url)\n",
    "image = Image.open(BytesIO(r.content))\n",
    "\n",
    "# Set the user prompt\n",
    "user_prompt = \"set in outer space\"\n",
    "\n",
    "# Set the style of SDXL\n",
    "user_style = {\n",
    "    \"name\": \"sai-cinematic\",\n",
    "    \"prompt\": \"cinematic film still {prompt} . shallow depth of field, vignette, highly detailed, high budget, bokeh, cinemascope, moody, epic, gorgeous, film grain, grainy\",\n",
    "    \"negative_prompt\": \"anime, cartoon, graphic, text, painting, crayon, graphite, abstract, glitch, deformed, mutated, ugly, disfigured\"\n",
    "}\n",
    "\n",
    "# Invoke OctoShop\n",
    "sdxl_image, clip_labels, llama2_output = octoshop(image, user_prompt, user_style)\n",
    "\n",
    "# Display the image\n",
    "print(\"CLIP Interrogator output: {}\".format(clip_labels))\n",
    "print(\"Llama 2 output: {}\".format(llama2_output))\n",
    "display(sdxl_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac413349",
   "metadata": {},
   "source": [
    "Let's jump back to Section 4 of Lab 1 instructions: https://dockercon23.octoml.ai/lab2/octoshop-controlnets/#section-4-stop-your-docker-container"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
