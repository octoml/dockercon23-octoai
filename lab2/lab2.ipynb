{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6743265-6f14-46a1-ae89-e5c0878b1833",
   "metadata": {},
   "source": [
    "# ControlNet enhanced OctoShop Pipeline\n",
    "In this iPython Notebook, \n",
    "* We'll test a self-authored SDXL+Container container locally\n",
    "* We'll then provide instructions update the model container running on an OctoAI endpoint\n",
    "* You'll go ahead and learn to utilise Controlnets in the SDXL pipeline to get amazing images based on input images\n",
    "* Finally you'll test your SDXL endpoint within a complete OctoShop pipeline, that now features a ControlNet and AI-based face swapping for better consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23f4af8-092c-4cfa-8232-ea0fd480e70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some useful libraries\n",
    "import requests\n",
    "import json\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from base64 import b64encode, b64decode\n",
    "from IPython.display import display\n",
    "\n",
    "# Let's import the OctoAI Python SDK\n",
    "from octoai.client import Client\n",
    "\n",
    "# A helper function that reads a PIL Image objects and returns a base 64 encoded string\n",
    "def encode_image(image: Image) -> str:\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=\"png\")\n",
    "    im_base64 = b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "    return im_base64\n",
    "\n",
    "# A helper function that reads a base64 encoded string and returns a PIL Image object\n",
    "def decode_image(image_str: str) -> Image:\n",
    "    return Image.open(BytesIO(b64decode(image_str)))\n",
    "\n",
    "# A helper function that rescales images to a resolution that is known to work great in SDXL\n",
    "def rescale_image(image: Image) -> Image:\n",
    "    w, h = image.size\n",
    "    if w == h:\n",
    "        width = 1024\n",
    "        height = 1024\n",
    "    elif w > h:\n",
    "        width = 1024\n",
    "        height = 1024 * h // w\n",
    "    else:\n",
    "        width = 1024 * w // h\n",
    "        height = 1024\n",
    "    image = image.resize((width, height))\n",
    "    return image\n",
    "\n",
    "# Initialize the OctoAI Client\n",
    "# This will make it easier to interface with the model containers\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e33674-f573-471f-90b1-5550be011cca",
   "metadata": {},
   "source": [
    "## A. Test your SDXL+ControlNet container locally\n",
    "Make sure you've completed Sections 1 and 2 of Lab 2 described in the README.md.\n",
    "\n",
    "As a recap, the SDXL model container takes as input a dictionary with the following keys:\n",
    "* `image` (string) - a base64-encoded image\n",
    "* `prompt` (string) - the SDXL text prompt\n",
    "* `negative_prompt` (string) - the SDXL text prompt\n",
    "* `guidance_scale` (float) - the guidance scale (a.k.a. the configuration scale) of SDXL\n",
    "* `num_inference_steps` (int) - the number of SDXL denoising steps\n",
    "* `seed` (int) - seed of the image generation\n",
    "* `controlnet_conditioning_scale` (float) - controls how strong of an effect the ControlNet has on SDXL generation\n",
    "* `control_guidance_start` (float) - on a scale from 0 to 1, determines when the ControlNet kicks in during the diffusion process\n",
    "* `control_guidance_end` (float) - on a scale from 0 to 1, determines when the ControlNet stops having an effect during the diffusion process\n",
    "\n",
    "SDXL model container returns the following as outputs:\n",
    "* `image` (string) - a base64-encoded image\n",
    "\n",
    "Note that now that we pass in an image to the SDXL+ControlNet model, we derive the SDXL image dimensions (width, height) from the SDXL input parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c45026b-ac76-4eb4-9ff0-73b144f2b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's grab the Docker logo\n",
    "r = requests.get('https://raw.githubusercontent.com/vegaluisjose/blob/main/docker.jpeg')\n",
    "image = Image.open(BytesIO(r.content))\n",
    "\n",
    "# Rescale the image\n",
    "image = rescale_image(image)\n",
    "\n",
    "# Display the Docker logo\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa91e6-077c-4cc0-a54e-1353ca37ee00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's prepare our SDXL+ControlNet inference endpoint payload\n",
    "# Intentionally we'll turn off the ControlNet, therefore the input image has no effect on the output image.\n",
    "# We hard code the user-prompt to \"an ultrarealistic photo of a whale with shipping containers on its back\" (no use of CLIP Interrogator)\n",
    "SDXL_payload = {\n",
    "    \"image\": encode_image(image),\n",
    "    \"prompt\": \"an ultrarealistic photo of a whale with shipping containers on its back\",\n",
    "    \"negative_prompt\": \"blurry photo, distortion, low-res, bad quality\",\n",
    "    \"num_inference_steps\": 20,\n",
    "    \"guidance_scale\": 7.5,\n",
    "    \"seed\": 1,\n",
    "    \"controlnet_conditioning_scale\": 0.0,  # Determines how strongly the ControlNet affects the generation process - let's start at 0\n",
    "    \"control_guidance_start\": 0.0,         # At 0, means start applying the ControlNet when 0% of the steps have completed\n",
    "    \"control_guidance_end\": 0.5,           # At 0.5, means stop applying the ControlNet when 50% of the steps have completed\n",
    "}\n",
    "\n",
    "# Run inference on the OctoAI SDXL+ControlNet model container running locally\n",
    "output = client.infer(\n",
    "    endpoint_url=\"http://localhost:8080/predict\",\n",
    "    inputs=SDXL_payload\n",
    ")\n",
    "\n",
    "# Get the base64 encoded image string\n",
    "image_string = output[\"completion\"][\"image\"]\n",
    "\n",
    "# Convert to a PIL image\n",
    "sdxl_image = decode_image(image_string)\n",
    "\n",
    "# Display your masterpiece!\n",
    "display(sdxl_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3072612d-c23c-4209-8d8c-63e6f24899ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a comparison, let's see what SDXL would generate if the ControlNet applied intentionally very strongly\n",
    "SDXL_payload[\"controlnet_conditioning_scale\"] = 1.25\n",
    "\n",
    "# Run inference on the OctoAI SDXL+ControlNet model container running locally\n",
    "output = client.infer(\n",
    "    endpoint_url=\"http://localhost:8080/predict\",\n",
    "    inputs=SDXL_payload\n",
    ")\n",
    "\n",
    "# Get the base64 encoded image string\n",
    "image_string = output[\"completion\"][\"image\"]\n",
    "\n",
    "# Convert to a PIL image\n",
    "sdxl_image = decode_image(image_string)\n",
    "\n",
    "# Display your masterpiece!\n",
    "display(sdxl_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9276387-9d8a-4d1b-96fd-fb1c57a4ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a comparison, let's see what SDXL would generate if the ControlNet was applied at \"just at the right\" strength\n",
    "SDXL_payload[\"controlnet_conditioning_scale\"] = 0.5\n",
    "\n",
    "# Run inference on the OctoAI SDXL+ControlNet model container running locally\n",
    "output = client.infer(\n",
    "    endpoint_url=\"http://localhost:8080/predict\",\n",
    "    inputs=SDXL_payload\n",
    ")\n",
    "\n",
    "# Get the base64 encoded image string\n",
    "image_string = output[\"completion\"][\"image\"]\n",
    "\n",
    "# Convert to a PIL image\n",
    "sdxl_image = decode_image(image_string)\n",
    "\n",
    "# Display your masterpiece!\n",
    "display(sdxl_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49c0a35-b503-4f80-bc22-f160b32e82f9",
   "metadata": {},
   "source": [
    "## B. Summary of the SDXL ControlNet experiments\n",
    "\n",
    "In this first part, we  evaluated the impact of ControlNets on SDXL-generated images:\n",
    "* At first, we turned off ControlNet entirely - this means that we are generating an image completely unconstrained by the input control image (the image of Moby).\n",
    "* Second, we turned the ControlNet conditioning scale all the way up. This applied a constraint that was a bit too strong, leading to a resulting image that looked too close to the input image (the image of the Docker logo). We lost all of the realism that we're trying to attain.\n",
    "* Finally, we set the ControlNet to just the right strength to get a photo that is both realistic and also close to the input control image (the image of Moby logo)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b39f2f-5f15-4657-a5b4-21882aeaba07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## C. Upload the image to your DockerHub\n",
    "Now sign onto your DockerHub in a browser: https://hub.docker.com/\n",
    "\n",
    "Create a repository by clicking on the `Create repository` blue button. Name it `dockercon-sdxl-canny`, and provide a short description as you see fit. Leave it public. Hit the `Create` blue button.\n",
    "\n",
    "Once that's done, note the full path to the repo, as `<dockerhub-username>/dockercon-sdxl-canny`.\n",
    "\n",
    "Under `dockercon23/lab2/dockercon-sdxl-canny`, run the following to tag the Docker image we just tested to a versioned image we'll push to the newly created DockerHub repository.\n",
    "```\n",
    "docker tag sdxl-canny:latest <dockerhub-username>/dockercon-sdxl-canny:v0.1.0\n",
    "```\n",
    "\n",
    "Then push the tagged SDXL Canny model image!\n",
    "\n",
    "```\n",
    "docker push <dockerhub-username>/dockercon-sdxl-canny:v0.1.0\n",
    "```\n",
    "\n",
    "This should take under 10 minutes to upload the container image given that the image is quite voluminous (that's pretty common for Generative AI models with their huge sets of weights!).\n",
    "\n",
    "Refresh the dockerhub page of the sdxl-canny repository, and you should see a new `v0.1.0` image that was uploaded just now!\n",
    "\n",
    "![Docker](https://raw.githubusercontent.com/vegaluisjose/blob/main/docker_sdxl_canny.png)\n",
    "\n",
    "If you don't feel like waiting for the full image to upload, you can go ahead and use this image that we've prebuilt for step D: [tmoreau89octo/dockercon-sdxl-canny:v0.1.0](https://hub.docker.com/layers/tmoreau89octo/dockercon-sdxl-canny/v0.1.0/images/sha256-ee91b22e1329f484593a9d59ff52c4a4bc751e7a80385a1da72ac359541d3490?context=repo).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478cb6b5-f6d4-4a93-ad7f-a1bf5b7ce61a",
   "metadata": {},
   "source": [
    "## D. Update the image on an already running OctoAI endpoint\n",
    "Sign onto your OctoAI account in a browser: https://octoai.cloud/endpoints\n",
    "\n",
    "Click on the endpoint that you created in Lab1, e.g. `dockercon23-sdxl`.\n",
    "\n",
    "Click on the `Edit endpoint` button.\n",
    "\n",
    "Edit the `Container image` under `Model container` to point to the new image you've just uploaded to DockerHub: `<dockerhub-username>/dockercon-sdxl-canny:v0.1.0`.\n",
    "\n",
    "The endpoint will swap the old SDXL container for the new SDXL-Canny container. This swap operation will take a few minutes the first time since it needs to download the container image from DockerHub, then start the container and initialize the SDXL+ControlNet pipeline.\n",
    "\n",
    "![OctoAI](https://raw.githubusercontent.com/vegaluisjose/blob/main/octoai_sdxl_canny.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1c18e-de6c-4530-8d71-dc890d3a5f82",
   "metadata": {},
   "source": [
    "## D. Test your SDXL container served on an OctoAI endpoint\n",
    "In this step, we'll test the SDXL container in the exact same way as we did when we ran the container locally on the AWS dev instance, except that now we'll be sending a POST request to a remote endpoint.\n",
    "\n",
    "You'll need to change the SDXL endpoint URL from `http://localhost:8080` to your unique endpoint URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac9c7bf-ae44-478f-8b99-227d708f6675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: Replace \"http://localhost:8080\" with your endpoint URL below\n",
    "sdxl_endpoint_url = \"http://localhost:8080\"\n",
    "# Make sure you've overwritten the URL!!!\n",
    "assert sdxl_endpoint_url != \"http://localhost:8080\"\n",
    "\n",
    "# Compared to Step A, we've replaced the http://localhost:8080 with\n",
    "# the URL of your newly launched OctoAI endpoint\n",
    "output = client.infer(\n",
    "    endpoint_url=\"{}/predict\".format(sdxl_endpoint_url),\n",
    "    inputs=SDXL_payload\n",
    ")\n",
    "\n",
    "# Get the base64 encoded image string\n",
    "image_string = output[\"completion\"][\"image\"]\n",
    "\n",
    "# Convert to a PIL image\n",
    "sdxl_image = decode_image(image_string)\n",
    "\n",
    "# Display your masterpiece!\n",
    "display(sdxl_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da55fb9e-d0a6-4b5b-9573-f772979a6abd",
   "metadata": {},
   "source": [
    "## G. Let's test the OctoShop workflow enhanced by ControlNets\n",
    "\n",
    "We are using the exact same workflow as in Lab 1 but it's been tweaked to make use of a ControlNet in the SDXL stage.\n",
    "\n",
    "1. User provides an image as input (docker logo) and a prompt (set in space).\n",
    "\n",
    "2. Image goes through CLIP interrogator and produces a text description of the image.\n",
    "\n",
    "3. The user prompt gets fed along with the CLIP interrogator description into LLAMA2 that describes a new scene.\n",
    "\n",
    "4. That textual description of the scene is fed into SDXL. SDXL image generation is constrained by a user-provided image (docker logo) via a ControlNet. This lets us generate a brand new photo of the Docker logo set in space that is more consistent with the original logo compared to Lab1!\n",
    "\n",
    "We've provided the whole flow below of OctoShop for you to play around with the Generative AI workflow! \n",
    "* Try changing the URL of the input image to a different image!\n",
    "* Try changing the user prompt to a different prompt!\n",
    "* Try changing the user style to a different style!\n",
    "* Try changing any combination of the above at the same time!\n",
    "* And feel free to tweak the various settings to familiarize yourself a bit more to the different models that are being invoked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8651f50-8e4c-45ea-9bab-b110796e711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define the OctoShop as a self-contained function\n",
    "def octoshop(image: Image, user_prompt: str, user_style: str) -> (Image, str, str):\n",
    "\n",
    "    # OctoAI endpoint URLs\n",
    "    clip_endpoint_url = \"https://dockercon23-clip-4jkxk521l3v1.octoai.run\"\n",
    "    llama2_endpoint_url = \"https://dockercon23-llama2-4jkxk521l3v1.octoai.run/v1/chat/completions\"\n",
    "    sdxl_endpoint_url = \"http://localhost:8080\" # ADD_YOUR_SDXL_ENDPOINT_URL_HERE\n",
    "    assert sdxl_endpoint_url != \"http://localhost:8080\"\n",
    "\n",
    "    # STEP 0\n",
    "    # Rescale the image\n",
    "    image = rescale_image(image)\n",
    "\n",
    "    # STEP 1\n",
    "    # Feed that image into CLIP interrogator\n",
    "    clip_request = {\n",
    "        \"mode\": \"fast\",\n",
    "        \"image\": encode_image(image),\n",
    "    }\n",
    "    output = client.infer(\n",
    "        endpoint_url=\"{}/predict\".format(clip_endpoint_url),\n",
    "        inputs=clip_request\n",
    "    )\n",
    "    clip_labels = output[\"completion\"][\"labels\"]\n",
    "    clip_labels = clip_labels.split(',')[0]\n",
    "\n",
    "    # STEP 2\n",
    "    # Feed that CLIP label and the user prompt into a LLAMA model\n",
    "    llama_prompt = \"\\\n",
    "    ### Instruction: In a single sentence, {}: {}\\\n",
    "    ### Response:\".format(user_prompt, clip_labels)\n",
    "    llama_inputs = {\n",
    "      \"model\": \"llama-2-7b-chat\",\n",
    "      \"messages\": [\n",
    "        {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "        },\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": \"{}\".format(llama_prompt)\n",
    "        }\n",
    "      ],\n",
    "      \"stream\": False,\n",
    "      \"max_tokens\": 256\n",
    "    }\n",
    "    outputs = client.infer(endpoint_url=llama2_endpoint_url, inputs=llama_inputs)\n",
    "    llama2_text = outputs.get('choices')[0].get(\"message\").get('content')\n",
    "\n",
    "    # STEP 3\n",
    "    # Feed the LLAMA2 text into the SDXL model\n",
    "    SDXL_payload = {\n",
    "        \"image\": encode_image(image),\n",
    "        \"prompt\": user_style[\"prompt\"].replace(\"{prompt}\", llama2_text),\n",
    "        \"negative_prompt\": user_style[\"negative_prompt\"],\n",
    "        \"num_inference_steps\": 20,\n",
    "        \"guidance_scale\": 7.5,\n",
    "        \"seed\": 1,\n",
    "        \"controlnet_conditioning_scale\": 0.5,\n",
    "        \"control_guidance_start\": 0.0,\n",
    "        \"control_guidance_end\": 0.5,\n",
    "    }\n",
    "    # Run inference on the OctoAI SDXL model container running locally\n",
    "    output = client.infer(\n",
    "        endpoint_url=\"{}/predict\".format(sdxl_endpoint_url),\n",
    "        inputs=SDXL_payload\n",
    "    )\n",
    "    image_string = output[\"completion\"][\"image\"]\n",
    "    sdxl_image = decode_image(image_string)\n",
    "\n",
    "    return sdxl_image, clip_labels, llama2_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a01520-08ae-48e4-9bdd-80ea649ee1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the to the image URL\n",
    "image_url = 'https://raw.githubusercontent.com/vegaluisjose/blob/main/docker.jpeg'\n",
    "\n",
    "# Process encode the input image into a string\n",
    "r = requests.get(image_url)\n",
    "image = Image.open(BytesIO(r.content))\n",
    "\n",
    "# Set the user prompt\n",
    "user_prompt = \"set in outer space\"\n",
    "\n",
    "# Set the style of SDXL\n",
    "user_style = {\n",
    "    \"name\": \"sai-cinematic\",\n",
    "    \"prompt\": \"cinematic film still {prompt} . shallow depth of field, vignette, highly detailed, high budget, bokeh, cinemascope, moody, epic, gorgeous, film grain, grainy\",\n",
    "    \"negative_prompt\": \"anime, cartoon, graphic, text, painting, crayon, graphite, abstract, glitch, deformed, mutated, ugly, disfigured\"\n",
    "}\n",
    "\n",
    "# Invoke OctoShop\n",
    "sdxl_image, clip_labels, llama2_output = octoshop(image, user_prompt, user_style)\n",
    "\n",
    "# Display the image\n",
    "print(\"CLIP Interrogator output: {}\".format(clip_labels))\n",
    "print(\"LLAMA2 output: {}\".format(llama2_output))\n",
    "display(sdxl_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a3d623-02cb-4126-a5d9-8e83ceef4059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try it with a person this time around\n",
    "image_url = 'https://media.snl.no/media/7650/standard_compressed_Mona_Lisa.jpg'\n",
    "r = requests.get(image_url)\n",
    "image = Image.open(BytesIO(r.content))\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7327b936-3bae-4240-8927-64d417d628bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke OctoShop\n",
    "sdxl_image, clip_labels, llama2_output = octoshop(image, user_prompt, user_style)\n",
    "\n",
    "# Display the image\n",
    "print(\"CLIP Interrogator output: {}\".format(clip_labels))\n",
    "print(\"LLAMA2 output: {}\".format(llama2_output))\n",
    "display(sdxl_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9764532-a01f-48b2-a42a-697bfa6d21a5",
   "metadata": {},
   "source": [
    "## F. Let's test the OctoShop workflow enhanced by AI-based FaceSwap\n",
    "\n",
    "ControlNet gives us pose consistency on people but the faces lose their likeness. There is a simple solution to this which is built on top of AI-based face swap. We brought up an endpoint that will let us perform AI-based faceswaps to bring the likeness of the mona lisa on our final result.\n",
    "\n",
    "For this DockerCon23 workshop, we've pre-allocated a face swap model endpoint pool available at the following URL: https://dockercon23-faceswap-4jkxk521l3v1.octoai.run\n",
    "\n",
    "\n",
    "**If you try this tutorial after October 3rd 2023**, this face swap endpoint will be taken down. You can still create and manage your own by going on https://octoai.cloud/endpoints\n",
    "* Click on the `Create a Custom Endpoint` blue button.\n",
    "* Name your endpoint, e.g. `dockercon23-faceswap`.\n",
    "* Under the `Model container` details:\n",
    "    * Set the `Container image` to `tmoreau89octo/faceswap:v0.1.3`\n",
    "    * Leave the `Container port` to its default `8080` value.\n",
    "    * Leave the `Registry credential` to `Public`.\n",
    "    * Set the `Health check path` to `/healthcheck`.\n",
    "    * Enable public access by toggling the switch (usually we'd recommend leaving it disabled but for the purpose of this lab, let's keep things simple).\n",
    "    * No need to specify secrets.\n",
    "    * No need to specify environment variables.\n",
    "* Under `Hardware tier`, select `Medium`.\n",
    "* Under `Configure autoscaling`:\n",
    "    * Change Min replicas to `1`. This will ensure at least one replica remains up and running.\n",
    "    * Change Max replicas to `1`. This will ensure no more than one replica remains up and running.\n",
    "    * Leave the timeout to `300` seconds.\n",
    "    * Now hit the `Create` button!\n",
    "\n",
    "![OctoAI](https://raw.githubusercontent.com/vegaluisjose/blob/main/octoai_faceswap.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8584841d-9052-40bc-a3a7-ec4057347516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a Face Swap function\n",
    "def faceswap(src: Image, dest: Image) -> (Image):\n",
    "\n",
    "    # OctoAI endpoint URLs\n",
    "    faceswap_endpoint_url = \"https://dockercon23-faceswap-4jkxk521l3v1.octoai.run\"\n",
    "\n",
    "    output = client.infer(\n",
    "        endpoint_url=\"{}/predict\".format(faceswap_endpoint_url),\n",
    "        inputs={\n",
    "            \"src_image\": encode_image(image),\n",
    "            \"dst_image\": encode_image(dest)\n",
    "        }\n",
    "    )\n",
    "    fs_sdxl_image_string = output[\"completion\"][\"image\"]\n",
    "    fs_sdxl_image = decode_image(fs_sdxl_image_string)\n",
    "\n",
    "    return fs_sdxl_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2401f4-5c48-470d-959b-ba2edf6b4928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the FaceSwap\n",
    "sdxl_fs_image = faceswap(image, sdxl_image)\n",
    "\n",
    "# Display the image\n",
    "display(sdxl_fs_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
